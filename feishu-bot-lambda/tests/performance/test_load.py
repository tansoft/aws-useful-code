"""
è´Ÿè½½å’Œæ€§èƒ½æµ‹è¯•
æµ‹è¯•ç³»ç»Ÿåœ¨é«˜è´Ÿè½½ä¸‹çš„è¡¨ç°
"""

import time
import json
import pytest
import threading
import concurrent.futures
from unittest.mock import Mock, patch
import psutil
import os

from src.lambdas.receive_handler import lambda_handler as receive_handler
from src.lambdas.process_handler import lambda_handler as process_handler
from src.shared.models import FeishuMessage


@pytest.mark.performance
class TestLoadPerformance:
    """è´Ÿè½½æ€§èƒ½æµ‹è¯•"""
    
    def test_high_volume_webhook_processing(self, feishu_config, lambda_context):
        """æµ‹è¯•é«˜å¹¶å‘webhookå¤„ç†"""
        # å‡†å¤‡å¤§é‡æµ‹è¯•äº‹ä»¶
        test_events = []
        for i in range(100):
            event = {
                'httpMethod': 'POST',
                'headers': {
                    'Content-Type': 'application/json',
                    'x-lark-request-timestamp': str(int(time.time())),
                    'x-lark-request-nonce': f'nonce_{i}',
                    'x-lark-signature': 'test_signature'
                },
                'body': json.dumps({
                    'header': {
                        'event_type': 'url_verification'
                    },
                    'challenge': f'challenge_{i}'
                })
            }
            test_events.append(event)
        
        # è®°å½•å¼€å§‹æ—¶é—´å’Œå†…å­˜
        start_time = time.time()
        process = psutil.Process(os.getpid())
        start_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # å¹¶å‘å¤„ç†äº‹ä»¶
        with patch('src.lambdas.receive_handler._verify_request_signature', return_value=True):
            with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
                futures = [
                    executor.submit(receive_handler, event, lambda_context)
                    for event in test_events
                ]
                
                results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        # è®°å½•ç»“æŸæ—¶é—´å’Œå†…å­˜
        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024  # MB
        
        # æ€§èƒ½æ–­è¨€
        processing_time = end_time - start_time
        memory_increase = end_memory - start_memory
        
        # éªŒè¯æ‰€æœ‰è¯·æ±‚éƒ½æˆåŠŸå¤„ç†
        assert len(results) == 100
        success_count = sum(1 for r in results if r['statusCode'] == 200)
        assert success_count >= 95  # è‡³å°‘95%æˆåŠŸç‡
        
        # æ€§èƒ½è¦æ±‚
        assert processing_time < 30.0  # 30ç§’å†…å®Œæˆ100ä¸ªè¯·æ±‚
        assert memory_increase < 200  # å†…å­˜å¢é•¿ä¸è¶…è¿‡200MB
        
        # è®¡ç®—ååé‡
        throughput = len(test_events) / processing_time
        print(f"Throughput: {throughput:.2f} requests/second")
        print(f"Average response time: {processing_time/len(test_events)*1000:.2f}ms")
        print(f"Memory increase: {memory_increase:.2f}MB")
        
        assert throughput > 3.0  # è‡³å°‘3 RPS
    
    def test_batch_message_processing_performance(self, feishu_config, lambda_context, mock_feishu_client):
        """æµ‹è¯•æ‰¹é‡æ¶ˆæ¯å¤„ç†æ€§èƒ½"""
        # åˆ›å»ºå¤§é‡æµ‹è¯•æ¶ˆæ¯
        batch_size = 50
        test_messages = []
        
        for i in range(batch_size):
            message = FeishuMessage(
                message_id=f'msg_{i}',
                user_id=f'user_{i}',
                chat_id=f'chat_{i % 10}',  # 10ä¸ªä¸åŒçš„èŠå¤©
                message_type='text',
                content=f'Performance test message {i}',
                timestamp=int(time.time()),
                app_id='test_app',
                mentions=[]
            )
            test_messages.append(message)
        
        # åˆ›å»ºSQSäº‹ä»¶
        sqs_event = {
            'Records': [
                {
                    'messageId': f'sqs_msg_{i}',
                    'body': msg.to_json()
                }
                for i, msg in enumerate(test_messages)
            ]
        }
        
        # è®°å½•æ€§èƒ½æŒ‡æ ‡
        start_time = time.time()
        process = psutil.Process(os.getpid())
        start_memory = process.memory_info().rss / 1024 / 1024
        
        # å¤„ç†æ¶ˆæ¯
        response = process_handler(sqs_event, lambda_context)
        
        end_time = time.time()
        end_memory = process.memory_info().rss / 1024 / 1024
        
        # éªŒè¯å¤„ç†ç»“æœ
        assert response['statusCode'] == 200
        assert response['body']['successful'] == batch_size
        assert response['body']['failed'] == 0
        
        # æ€§èƒ½è¦æ±‚
        processing_time = end_time - start_time
        memory_increase = end_memory - start_memory
        
        assert processing_time < 15.0  # 15ç§’å†…å¤„ç†50æ¡æ¶ˆæ¯
        assert memory_increase < 100  # å†…å­˜å¢é•¿ä¸è¶…è¿‡100MB
        
        # éªŒè¯é£ä¹¦APIè°ƒç”¨æ¬¡æ•°
        assert mock_feishu_client.send_text_message.call_count == batch_size
        
        # è®¡ç®—å¤„ç†é€Ÿåº¦
        messages_per_second = batch_size / processing_time
        print(f"Message processing rate: {messages_per_second:.2f} messages/second")
        print(f"Average processing time per message: {processing_time/batch_size*1000:.2f}ms")
        
        assert messages_per_second > 3.0  # è‡³å°‘3æ¡æ¶ˆæ¯/ç§’
    
    def test_memory_leak_detection(self, feishu_config, lambda_context):
        """æµ‹è¯•å†…å­˜æ³„æ¼æ£€æµ‹"""
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024
        memory_readings = []
        
        # å¤šæ¬¡æ‰§è¡Œç›¸åŒæ“ä½œ
        for iteration in range(15):  # å¢åŠ è¿­ä»£æ¬¡æ•°
            # åˆ›å»ºæµ‹è¯•äº‹ä»¶
            events = []
            for i in range(30):  # å¢åŠ æ¯æ¬¡è¿­ä»£çš„äº‹ä»¶æ•°é‡
                event = {
                    'httpMethod': 'POST',
                    'headers': {
                        'x-lark-request-timestamp': str(int(time.time())),
                        'x-lark-request-nonce': f'nonce_{iteration}_{i}',
                        'x-lark-signature': 'test_signature'
                    },
                    'body': json.dumps({
                        'header': {'event_type': 'url_verification'},
                        'challenge': f'challenge_{iteration}_{i}'
                    })
                }
                events.append(event)
            
            # å¤„ç†äº‹ä»¶
            with patch('src.lambdas.receive_handler._verify_request_signature', return_value=True):
                for event in events:
                    receive_handler(event, lambda_context)
            
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            import gc
            gc.collect()
            
            # è®°å½•å†…å­˜ä½¿ç”¨
            current_memory = process.memory_info().rss / 1024 / 1024
            memory_increase = current_memory - initial_memory
            memory_readings.append(current_memory)
            
            print(f"Iteration {iteration + 1}: Memory usage = {current_memory:.2f}MB, "
                  f"Increase = {memory_increase:.2f}MB")
            
            # å†…å­˜å¢é•¿åº”è¯¥ä¿æŒåœ¨åˆç†èŒƒå›´å†…
            assert memory_increase < 100  # ä¸è¶…è¿‡100MBå¢é•¿
        
        # æ£€æŸ¥å†…å­˜å¢é•¿è¶‹åŠ¿
        if len(memory_readings) >= 10:
            # è®¡ç®—ååŠæ®µçš„å¹³å‡å†…å­˜ä½¿ç”¨
            recent_avg = sum(memory_readings[-5:]) / 5
            early_avg = sum(memory_readings[:5]) / 5
            
            # å†…å­˜å¢é•¿åº”è¯¥è¶‹äºç¨³å®š
            growth_rate = (recent_avg - early_avg) / early_avg
            assert growth_rate < 0.5  # å¢é•¿ç‡ä¸è¶…è¿‡50%
            
            print(f"Memory growth rate: {growth_rate:.2%}")
    
    def test_memory_usage_under_stress(self, feishu_config, lambda_context):
        """æµ‹è¯•å‹åŠ›ä¸‹çš„å†…å­˜ä½¿ç”¨"""
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024
        
        # åˆ›å»ºå¤§é‡å¹¶å‘äº‹ä»¶
        import threading
        import queue
        
        results_queue = queue.Queue()
        
        def stress_worker(worker_id):
            """å‹åŠ›æµ‹è¯•å·¥ä½œçº¿ç¨‹"""
            for i in range(50):
                event = {
                    'httpMethod': 'POST',
                    'headers': {
                        'x-lark-request-timestamp': str(int(time.time())),
                        'x-lark-request-nonce': f'stress_{worker_id}_{i}',
                        'x-lark-signature': 'test_signature'
                    },
                    'body': json.dumps({
                        'header': {'event_type': 'url_verification'},
                        'challenge': f'stress_challenge_{worker_id}_{i}'
                    })
                }
                
                with patch('src.lambdas.receive_handler._verify_request_signature', return_value=True):
                    try:
                        response = receive_handler(event, lambda_context)
                        results_queue.put(('success', response['statusCode']))
                    except Exception as e:
                        results_queue.put(('error', str(e)))
        
        # å¯åŠ¨å¤šä¸ªå‹åŠ›æµ‹è¯•çº¿ç¨‹
        threads = []
        for worker_id in range(10):
            thread = threading.Thread(target=stress_worker, args=(worker_id,))
            threads.append(thread)
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        # æ£€æŸ¥æœ€ç»ˆå†…å­˜ä½¿ç”¨
        final_memory = process.memory_info().rss / 1024 / 1024
        memory_increase = final_memory - initial_memory
        
        print(f"Stress test memory increase: {memory_increase:.2f}MB")
        
        # æ”¶é›†ç»“æœ
        success_count = 0
        error_count = 0
        
        while not results_queue.empty():
            result_type, result_data = results_queue.get()
            if result_type == 'success':
                success_count += 1
            else:
                error_count += 1
        
        # éªŒè¯ç»“æœ
        total_requests = 10 * 50  # 10ä¸ªçº¿ç¨‹ï¼Œæ¯ä¸ª50ä¸ªè¯·æ±‚
        assert success_count + error_count == total_requests
        assert success_count >= total_requests * 0.95  # è‡³å°‘95%æˆåŠŸç‡
        assert memory_increase < 200  # å†…å­˜å¢é•¿ä¸è¶…è¿‡200MB
    
    def test_concurrent_different_operations(self, feishu_config, lambda_context, mock_feishu_client):
        """æµ‹è¯•ä¸åŒæ“ä½œçš„å¹¶å‘å¤„ç†"""
        import queue
        
        results_queue = queue.Queue()
        
        def webhook_worker():
            """Webhookå¤„ç†å·¥ä½œçº¿ç¨‹"""
            event = {
                'httpMethod': 'POST',
                'headers': {
                    'x-lark-request-timestamp': str(int(time.time())),
                    'x-lark-request-nonce': 'webhook_nonce',
                    'x-lark-signature': 'test_signature'
                },
                'body': json.dumps({
                    'header': {'event_type': 'url_verification'},
                    'challenge': 'webhook_challenge'
                })
            }
            
            with patch('src.lambdas.receive_handler._verify_request_signature', return_value=True):
                result = receive_handler(event, lambda_context)
                results_queue.put(('webhook', result))
        
        def message_worker():
            """æ¶ˆæ¯å¤„ç†å·¥ä½œçº¿ç¨‹"""
            message = FeishuMessage(
                message_id='concurrent_msg',
                user_id='concurrent_user',
                chat_id='concurrent_chat',
                message_type='text',
                content='Concurrent test message',
                timestamp=int(time.time()),
                app_id='test_app',
                mentions=[]
            )
            
            sqs_event = {
                'Records': [
                    {
                        'messageId': 'concurrent_sqs_msg',
                        'body': message.to_json()
                    }
                ]
            }
            
            result = process_handler(sqs_event, lambda_context)
            results_queue.put(('message', result))
        
        # å¯åŠ¨å¹¶å‘å·¥ä½œçº¿ç¨‹
        threads = []
        
        # åˆ›å»ºå¤šä¸ªwebhookå¤„ç†çº¿ç¨‹
        for _ in range(5):
            thread = threading.Thread(target=webhook_worker)
            threads.append(thread)
        
        # åˆ›å»ºå¤šä¸ªæ¶ˆæ¯å¤„ç†çº¿ç¨‹
        for _ in range(5):
            thread = threading.Thread(target=message_worker)
            threads.append(thread)
        
        # å¯åŠ¨æ‰€æœ‰çº¿ç¨‹
        start_time = time.time()
        for thread in threads:
            thread.start()
        
        # ç­‰å¾…æ‰€æœ‰çº¿ç¨‹å®Œæˆ
        for thread in threads:
            thread.join()
        
        end_time = time.time()
        
        # æ”¶é›†ç»“æœ
        results = []
        while not results_queue.empty():
            results.append(results_queue.get())
        
        # éªŒè¯ç»“æœ
        assert len(results) == 10  # 5ä¸ªwebhook + 5ä¸ªæ¶ˆæ¯å¤„ç†
        
        webhook_results = [r for r in results if r[0] == 'webhook']
        message_results = [r for r in results if r[0] == 'message']
        
        assert len(webhook_results) == 5
        assert len(message_results) == 5
        
        # éªŒè¯æ‰€æœ‰æ“ä½œéƒ½æˆåŠŸ
        for _, result in results:
            assert result['statusCode'] == 200
        
        # æ€§èƒ½è¦æ±‚
        total_time = end_time - start_time
        assert total_time < 10.0  # 10ç§’å†…å®Œæˆæ‰€æœ‰å¹¶å‘æ“ä½œ
        
        print(f"Concurrent operations completed in {total_time:.2f} seconds")


@pytest.mark.performance
class TestScalabilityLimits:
    """å¯æ‰©å±•æ€§é™åˆ¶æµ‹è¯•"""
    
    def test_maximum_message_size_handling(self, feishu_config, lambda_context):
        """æµ‹è¯•æœ€å¤§æ¶ˆæ¯å¤§å°å¤„ç†"""
        # åˆ›å»ºå¤§æ¶ˆæ¯ï¼ˆæ¥è¿‘Lambdaé™åˆ¶ï¼‰
        large_content = 'x' * (5 * 1024 * 1024)  # 5MBå†…å®¹
        
        large_event = {
            'httpMethod': 'POST',
            'headers': {
                'x-lark-request-timestamp': str(int(time.time())),
                'x-lark-request-nonce': 'large_nonce',
                'x-lark-signature': 'test_signature'
            },
            'body': json.dumps({
                'header': {
                    'event_type': 'im.message.receive_v1',
                    'app_id': 'test_app'
                },
                'event': {
                    'message': {
                        'content': f'{{"text": "{large_content}"}}'
                    }
                }
            })
        }
        
        start_time = time.time()
        
        with patch('src.lambdas.receive_handler._verify_request_signature', return_value=True):
            response = receive_handler(large_event, lambda_context)
        
        end_time = time.time()
        processing_time = end_time - start_time
        
        # åº”è¯¥èƒ½å¤Ÿå¤„ç†å¤§æ¶ˆæ¯ï¼ˆå¯èƒ½è¿”å›é”™è¯¯ï¼Œä½†ä¸åº”è¯¥å´©æºƒï¼‰
        assert response['statusCode'] in [200, 400, 413]  # 200æˆåŠŸ, 400é”™è¯¯, 413è¿‡å¤§
        assert processing_time < 30.0  # 30ç§’å†…å®Œæˆå¤„ç†
    
    def test_deep_json_nesting_handling(self, feishu_config, lambda_context):
        """æµ‹è¯•æ·±åº¦åµŒå¥—JSONå¤„ç†"""
        # åˆ›å»ºæ·±åº¦åµŒå¥—çš„JSON
        nested_data = {}
        current = nested_data
        
        for i in range(100):  # 100å±‚åµŒå¥—
            current['level'] = i
            current['next'] = {}
            current = current['next']
        
        current['final'] = 'deep_value'
        
        deep_event = {
            'httpMethod': 'POST',
            'headers': {
                'x-lark-request-timestamp': str(int(time.time())),
                'x-lark-request-nonce': 'deep_nonce',
                'x-lark-signature': 'test_signature'
            },
            'body': json.dumps({
                'header': {'event_type': 'url_verification'},
                'challenge': 'deep_challenge',
                'nested_data': nested_data
            })
        }
        
        with patch('src.lambdas.receive_handler._verify_request_signature', return_value=True):
            response = receive_handler(deep_event, lambda_context)
        
        # åº”è¯¥èƒ½å¤Ÿå¤„ç†æ·±åº¦åµŒå¥—ï¼ˆå¯èƒ½æœ‰é™åˆ¶ï¼Œä½†ä¸åº”è¯¥å´©æºƒï¼‰
        assert response['statusCode'] in [200, 400]
    
    def test_unicode_and_emoji_handling_performance(self, feishu_config, lambda_context, mock_feishu_client):
        """æµ‹è¯•Unicodeå’Œè¡¨æƒ…ç¬¦å·å¤„ç†æ€§èƒ½"""
        # åˆ›å»ºåŒ…å«å¤§é‡Unicodeå­—ç¬¦çš„æ¶ˆæ¯
        unicode_content = 'ğŸ‰ğŸš€ğŸ’»ğŸ”¥â­ï¸ğŸ¯ğŸŒŸğŸ’¡ğŸŠğŸˆ' * 1000  # å¤§é‡è¡¨æƒ…ç¬¦å·
        chinese_content = 'è¿™æ˜¯ä¸€ä¸ªåŒ…å«ä¸­æ–‡å­—ç¬¦çš„æµ‹è¯•æ¶ˆæ¯ï¼Œç”¨äºéªŒè¯Unicodeå¤„ç†æ€§èƒ½ã€‚' * 100
        mixed_content = f'{unicode_content} {chinese_content}'
        
        message = FeishuMessage(
            message_id='unicode_msg',
            user_id='unicode_user',
            chat_id='unicode_chat',
            message_type='text',
            content=mixed_content,
            timestamp=int(time.time()),
            app_id='test_app',
            mentions=[]
        )
        
        sqs_event = {
            'Records': [
                {
                    'messageId': 'unicode_sqs_msg',
                    'body': message.to_json()
                }
            ]
        }
        
        start_time = time.time()
        response = process_handler(sqs_event, lambda_context)
        end_time = time.time()
        
        processing_time = end_time - start_time
        
        # éªŒè¯å¤„ç†ç»“æœ
        assert response['statusCode'] == 200
        assert response['body']['successful'] == 1
        
        # Unicodeå¤„ç†ä¸åº”è¯¥æ˜¾è‘—å½±å“æ€§èƒ½
        assert processing_time < 5.0  # 5ç§’å†…å®Œæˆ
        
        # éªŒè¯é£ä¹¦APIè¢«æ­£ç¡®è°ƒç”¨
        mock_feishu_client.send_text_message.assert_called_once()
        call_args = mock_feishu_client.send_text_message.call_args
        assert 'ğŸ‰' in call_args[0][1]  # ç¡®ä¿è¡¨æƒ…ç¬¦å·è¢«ä¿ç•™
        assert 'ä¸­æ–‡' in call_args[0][1]  # ç¡®ä¿ä¸­æ–‡å­—ç¬¦è¢«ä¿ç•™


if __name__ == '__main__':
    pytest.main([__file__, '-v', '-m', 'performance'])
@p
ytest.mark.performance
class TestAdvancedPerformance:
    """é«˜çº§æ€§èƒ½æµ‹è¯•"""
    
    def test_cpu_intensive_operations(self, feishu_config, lambda_context, mock_feishu_client):
        """æµ‹è¯•CPUå¯†é›†å‹æ“ä½œçš„æ€§èƒ½"""
        import time
        import threading
        
        # åˆ›å»ºCPUå¯†é›†å‹çš„æ¶ˆæ¯å¤„ç†ä»»åŠ¡
        cpu_intensive_messages = []
        for i in range(20):
            # åˆ›å»ºåŒ…å«å¤æ‚å†…å®¹çš„æ¶ˆæ¯
            complex_content = json.dumps({
                "text": f"Complex message {i} with nested data",
                "metadata": {
                    "numbers": list(range(100)),
                    "nested": {
                        "level1": {
                            "level2": {
                                "data": [{"id": j, "value": f"item_{j}"} for j in range(50)]
                            }
                        }
                    }
                }
            })
            
            message = FeishuMessage(
                message_id=f'cpu_msg_{i}',
                user_id=f'cpu_user_{i}',
                chat_id=f'cpu_chat_{i}',
                message_type='text',
                content=complex_content,
                timestamp=int(time.time()),
                app_id='test_app',
                mentions=[]
            )
            cpu_intensive_messages.append(message)
        
        # åˆ›å»ºSQSäº‹ä»¶
        sqs_event = {
            'Records': [
                {
                    'messageId': f'cpu_sqs_msg_{i}',
                    'body': msg.to_json()
                }
                for i, msg in enumerate(cpu_intensive_messages)
            ]
        }
        
        # æµ‹é‡CPUä½¿ç”¨ç‡
        import psutil
        process = psutil.Process(os.getpid())
        
        start_time = time.time()
        start_cpu_percent = process.cpu_percent()
        
        # å¤„ç†æ¶ˆæ¯
        response = process_handler(sqs_event, lambda_context)
        
        end_time = time.time()
        end_cpu_percent = process.cpu_percent()
        
        processing_time = end_time - start_time
        
        # éªŒè¯å¤„ç†ç»“æœ
        assert response['statusCode'] == 200
        assert response['body']['successful'] == 20
        
        # æ€§èƒ½è¦æ±‚
        assert processing_time < 30.0  # 30ç§’å†…å®Œæˆ
        
        # CPUä½¿ç”¨ç‡åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
        print(f"CPU usage: {end_cpu_percent:.2f}%")
        print(f"Processing time: {processing_time:.2f}s")
        print(f"Messages per second: {20/processing_time:.2f}")
        
        # éªŒè¯é£ä¹¦APIè°ƒç”¨
        assert mock_feishu_client.send_text_message.call_count == 20
    
    def test_io_intensive_operations(self, feishu_config, lambda_context):
        """æµ‹è¯•I/Oå¯†é›†å‹æ“ä½œçš„æ€§èƒ½"""
        import concurrent.futures
        import time
        
        # æ¨¡æ‹ŸI/Oå¯†é›†å‹æ“ä½œï¼ˆç½‘ç»œè¯·æ±‚ï¼‰
        def simulate_io_operation(message_id):
            """æ¨¡æ‹ŸI/Oæ“ä½œ"""
            # æ¨¡æ‹Ÿç½‘ç»œå»¶è¿Ÿ
            time.sleep(0.1)  # 100mså»¶è¿Ÿ
            
            message_data = {
                'message_id': message_id,
                'user_id': f'io_user_{message_id}',
                'chat_id': f'io_chat_{message_id}',
                'message_type': 'text',
                'content': f'IO intensive message {message_id}',
                'timestamp': int(time.time()),
                'app_id': 'test_app',
                'mentions': []
            }
            
            sqs_event = {
                'Records': [
                    {
                        'messageId': f'io_sqs_msg_{message_id}',
                        'body': json.dumps(message_data)
                    }
                ]
            }
            
            # Mocké£ä¹¦APIè°ƒç”¨
            with patch('src.shared.feishu_client.FeishuClient.send_text_message') as mock_send:
                mock_send.return_value = {'code': 0, 'msg': 'success'}
                
                start_time = time.time()
                response = process_handler(sqs_event, lambda_context)
                end_time = time.time()
                
                return {
                    'message_id': message_id,
                    'response': response,
                    'processing_time': end_time - start_time
                }
        
        # å¹¶å‘æ‰§è¡ŒI/Oæ“ä½œ
        start_time = time.time()
        
        with concurrent.futures.ThreadPoolExecutor(max_workers=10) as executor:
            futures = [
                executor.submit(simulate_io_operation, f'io_msg_{i}')
                for i in range(50)
            ]
            
            results = [future.result() for future in concurrent.futures.as_completed(futures)]
        
        end_time = time.time()
        total_time = end_time - start_time
        
        # éªŒè¯ç»“æœ
        assert len(results) == 50
        
        successful_results = [r for r in results if r['response']['statusCode'] == 200]
        assert len(successful_results) == 50
        
        # æ€§èƒ½åˆ†æ
        avg_processing_time = sum(r['processing_time'] for r in results) / len(results)
        throughput = len(results) / total_time
        
        print(f"Total time: {total_time:.2f}s")
        print(f"Average processing time per message: {avg_processing_time:.2f}s")
        print(f"Throughput: {throughput:.2f} messages/second")
        
        # æ€§èƒ½è¦æ±‚
        assert total_time < 15.0  # å¹¶å‘å¤„ç†åº”è¯¥æ˜¾è‘—å¿«äºä¸²è¡Œ
        assert throughput > 3.0   # è‡³å°‘3 messages/second
    
    def test_memory_efficiency_with_large_batches(self, feishu_config, lambda_context, mock_feishu_client):
        """æµ‹è¯•å¤§æ‰¹é‡å¤„ç†çš„å†…å­˜æ•ˆç‡"""
        import psutil
        import gc
        
        process = psutil.Process(os.getpid())
        initial_memory = process.memory_info().rss / 1024 / 1024
        
        # åˆ›å»ºå¤§æ‰¹é‡æ¶ˆæ¯
        batch_sizes = [10, 50, 100, 200]
        memory_usage_per_batch = []
        
        for batch_size in batch_sizes:
            # å¼ºåˆ¶åƒåœ¾å›æ”¶
            gc.collect()
            
            # è®°å½•æ‰¹å¤„ç†å‰çš„å†…å­˜
            before_memory = process.memory_info().rss / 1024 / 1024
            
            # åˆ›å»ºæ‰¹é‡æ¶ˆæ¯
            messages = []
            for i in range(batch_size):
                message = FeishuMessage(
                    message_id=f'batch_msg_{batch_size}_{i}',
                    user_id=f'batch_user_{i}',
                    chat_id=f'batch_chat_{i}',
                    message_type='text',
                    content=f'Batch message {i} in batch of {batch_size}',
                    timestamp=int(time.time()),
                    app_id='test_app',
                    mentions=[]
                )
                messages.append(message)
            
            # åˆ›å»ºSQSäº‹ä»¶
            sqs_event = {
                'Records': [
                    {
                        'messageId': f'batch_sqs_{batch_size}_{i}',
                        'body': msg.to_json()
                    }
                    for i, msg in enumerate(messages)
                ]
            }
            
            # å¤„ç†æ‰¹é‡æ¶ˆæ¯
            start_time = time.time()
            response = process_handler(sqs_event, lambda_context)
            end_time = time.time()
            
            # è®°å½•æ‰¹å¤„ç†åçš„å†…å­˜
            after_memory = process.memory_info().rss / 1024 / 1024
            memory_used = after_memory - before_memory
            
            # éªŒè¯å¤„ç†ç»“æœ
            assert response['statusCode'] == 200
            assert response['body']['successful'] == batch_size
            
            # è®°å½•æ€§èƒ½æŒ‡æ ‡
            processing_time = end_time - start_time
            memory_per_message = memory_used / batch_size if batch_size > 0 else 0
            
            memory_usage_per_batch.append({
                'batch_size': batch_size,
                'memory_used': memory_used,
                'memory_per_message': memory_per_message,
                'processing_time': processing_time,
                'messages_per_second': batch_size / processing_time
            })
            
            print(f"Batch size {batch_size}: "
                  f"Memory used: {memory_used:.2f}MB, "
                  f"Per message: {memory_per_message:.3f}MB, "
                  f"Time: {processing_time:.2f}s, "
                  f"Rate: {batch_size/processing_time:.2f} msg/s")
            
            # å†…å­˜ä½¿ç”¨åº”è¯¥éšæ‰¹é‡å¤§å°çº¿æ€§å¢é•¿
            assert memory_used < batch_size * 0.5  # æ¯æ¡æ¶ˆæ¯ä¸è¶…è¿‡0.5MB
        
        # åˆ†æå†…å­˜æ•ˆç‡è¶‹åŠ¿
        if len(memory_usage_per_batch) >= 2:
            # æ£€æŸ¥å†…å­˜ä½¿ç”¨æ˜¯å¦éšæ‰¹é‡å¤§å°åˆç†å¢é•¿
            for i in range(1, len(memory_usage_per_batch)):
                current = memory_usage_per_batch[i]
                previous = memory_usage_per_batch[i-1]
                
                # å†…å­˜ä½¿ç”¨åº”è¯¥å¤§è‡´ä¸æ‰¹é‡å¤§å°æˆæ­£æ¯”
                size_ratio = current['batch_size'] / previous['batch_size']
                memory_ratio = current['memory_used'] / previous['memory_used'] if previous['memory_used'] > 0 else 1
                
                # å…è®¸ä¸€å®šçš„åå·®
                assert 0.5 < memory_ratio / size_ratio < 2.0
    
    def test_error_handling_performance_impact(self, feishu_config, lambda_context):
        """æµ‹è¯•é”™è¯¯å¤„ç†å¯¹æ€§èƒ½çš„å½±å“"""
        import time
        
        # åˆ›å»ºæ··åˆæˆåŠŸå’Œå¤±è´¥çš„æ¶ˆæ¯
        mixed_messages = []
        for i in range(100):
            message = FeishuMessage(
                message_id=f'mixed_msg_{i}',
                user_id=f'mixed_user_{i}',
                chat_id=f'mixed_chat_{i}',
                message_type='text',
                content=f'Mixed message {i}',
                timestamp=int(time.time()),
                app_id='test_app',
                mentions=[]
            )
            mixed_messages.append(message)
        
        # æµ‹è¯•1: å…¨éƒ¨æˆåŠŸçš„æƒ…å†µ
        sqs_event_success = {
            'Records': [
                {
                    'messageId': f'success_sqs_msg_{i}',
                    'body': msg.to_json()
                }
                for i, msg in enumerate(mixed_messages[:50])
            ]
        }
        
        with patch('src.shared.feishu_client.FeishuClient.send_text_message') as mock_send:
            mock_send.return_value = {'code': 0, 'msg': 'success'}
            
            start_time = time.time()
            response_success = process_handler(sqs_event_success, lambda_context)
            success_time = time.time() - start_time
        
        # æµ‹è¯•2: éƒ¨åˆ†å¤±è´¥çš„æƒ…å†µ
        sqs_event_mixed = {
            'Records': [
                {
                    'messageId': f'mixed_sqs_msg_{i}',
                    'body': msg.to_json()
                }
                for i, msg in enumerate(mixed_messages[50:])
            ]
        }
        
        with patch('src.shared.feishu_client.FeishuClient.send_text_message') as mock_send:
            # æ¨¡æ‹Ÿ50%å¤±è´¥ç‡
            mock_send.side_effect = [
                {'code': 0, 'msg': 'success'} if i % 2 == 0 else Exception("API Error")
                for i in range(50)
            ]
            
            # Mock time.sleep to speed up retry tests
            with patch('time.sleep'):
                start_time = time.time()
                response_mixed = process_handler(sqs_event_mixed, lambda_context)
                mixed_time = time.time() - start_time
        
        # éªŒè¯ç»“æœ
        assert response_success['statusCode'] == 200
        assert response_success['body']['successful'] == 50
        
        assert response_mixed['statusCode'] == 200
        # éƒ¨åˆ†æ¶ˆæ¯åº”è¯¥æˆåŠŸï¼Œéƒ¨åˆ†å¤±è´¥
        assert response_mixed['body']['successful'] > 0
        assert response_mixed['body']['failed'] > 0
        
        # æ€§èƒ½åˆ†æ
        success_rate = 50 / success_time
        mixed_rate = 50 / mixed_time
        
        print(f"Success-only processing rate: {success_rate:.2f} msg/s")
        print(f"Mixed success/failure processing rate: {mixed_rate:.2f} msg/s")
        print(f"Performance impact of errors: {((success_time - mixed_time) / success_time * 100):.1f}%")
        
        # é”™è¯¯å¤„ç†ä¸åº”è¯¥æ˜¾è‘—å½±å“æ€§èƒ½ï¼ˆè€ƒè™‘åˆ°é‡è¯•æœºåˆ¶ï¼‰
        # å…è®¸é”™è¯¯æƒ…å†µä¸‹æ€§èƒ½ä¸‹é™ï¼Œä½†ä¸åº”è¯¥è¶…è¿‡5å€
        assert mixed_time < success_time * 5
    
    def test_garbage_collection_impact(self, feishu_config, lambda_context, mock_feishu_client):
        """æµ‹è¯•åƒåœ¾å›æ”¶å¯¹æ€§èƒ½çš„å½±å“"""
        import gc
        import time
        
        # ç¦ç”¨è‡ªåŠ¨åƒåœ¾å›æ”¶
        gc.disable()
        
        try:
            # åˆ›å»ºå¤§é‡å¯¹è±¡ä»¥è§¦å‘åƒåœ¾å›æ”¶
            large_messages = []
            for i in range(500):
                # åˆ›å»ºåŒ…å«å¤§é‡æ•°æ®çš„æ¶ˆæ¯
                large_content = {
                    "text": f"Large message {i}",
                    "data": ["item_" + str(j) for j in range(100)],
                    "metadata": {f"key_{k}": f"value_{k}" for k in range(50)}
                }
                
                message = FeishuMessage(
                    message_id=f'gc_msg_{i}',
                    user_id=f'gc_user_{i}',
                    chat_id=f'gc_chat_{i}',
                    message_type='text',
                    content=json.dumps(large_content),
                    timestamp=int(time.time()),
                    app_id='test_app',
                    mentions=[]
                )
                large_messages.append(message)
            
            # åˆ†æ‰¹å¤„ç†æ¶ˆæ¯
            batch_size = 50
            processing_times = []
            
            for batch_start in range(0, len(large_messages), batch_size):
                batch_end = min(batch_start + batch_size, len(large_messages))
                batch_messages = large_messages[batch_start:batch_end]
                
                sqs_event = {
                    'Records': [
                        {
                            'messageId': f'gc_sqs_msg_{i}',
                            'body': msg.to_json()
                        }
                        for i, msg in enumerate(batch_messages)
                    ]
                }
                
                # æµ‹é‡å¤„ç†æ—¶é—´
                start_time = time.time()
                response = process_handler(sqs_event, lambda_context)
                end_time = time.time()
                
                processing_time = end_time - start_time
                processing_times.append(processing_time)
                
                # éªŒè¯å¤„ç†ç»“æœ
                assert response['statusCode'] == 200
                assert response['body']['successful'] == len(batch_messages)
                
                # æ¯éš”å‡ æ‰¹æ‰‹åŠ¨è§¦å‘åƒåœ¾å›æ”¶
                if (batch_start // batch_size) % 3 == 0:
                    gc_start = time.time()
                    gc.collect()
                    gc_time = time.time() - gc_start
                    print(f"Manual GC took {gc_time:.3f}s")
            
            # åˆ†æå¤„ç†æ—¶é—´çš„å˜åŒ–
            avg_time = sum(processing_times) / len(processing_times)
            max_time = max(processing_times)
            min_time = min(processing_times)
            
            print(f"Processing times - Avg: {avg_time:.3f}s, Min: {min_time:.3f}s, Max: {max_time:.3f}s")
            print(f"Time variation: {((max_time - min_time) / avg_time * 100):.1f}%")
            
            # å¤„ç†æ—¶é—´çš„å˜åŒ–åº”è¯¥åœ¨åˆç†èŒƒå›´å†…
            assert max_time < avg_time * 3  # æœ€å¤§æ—¶é—´ä¸è¶…è¿‡å¹³å‡æ—¶é—´çš„3å€
            
        finally:
            # é‡æ–°å¯ç”¨è‡ªåŠ¨åƒåœ¾å›æ”¶
            gc.enable()


if __name__ == '__main__':
    pytest.main([__file__, '-v', '-m', 'performance'])