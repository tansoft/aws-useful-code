#!/usr/bin/env python3
"""
æ€§èƒ½åŸºå‡†æµ‹è¯•è„šæœ¬

æ­¤è„šæœ¬ç”¨äºæµ‹è¯•é£ä¹¦æœºå™¨äººç³»ç»Ÿçš„æ€§èƒ½æŒ‡æ ‡ï¼ŒåŒ…æ‹¬å“åº”æ—¶é—´ã€ååé‡ã€å¹¶å‘å¤„ç†èƒ½åŠ›ç­‰ã€‚
"""

import json
import time
import asyncio
import aiohttp
import hashlib
import statistics
import logging
import argparse
from typing import Dict, List, Any, Optional
from dataclasses import dataclass
from datetime import datetime
from concurrent.futures import ThreadPoolExecutor, as_completed

# é…ç½®æ—¥å¿—
logging.basicConfig(
    level=logging.INFO,
    format='%(asctime)s - %(levelname)s - %(message)s'
)
logger = logging.getLogger(__name__)

@dataclass
class PerformanceMetric:
    """æ€§èƒ½æŒ‡æ ‡"""
    metric_name: str
    value: float
    unit: str
    description: str
    threshold: Optional[float] = None
    passed: Optional[bool] = None

@dataclass
class BenchmarkResult:
    """åŸºå‡†æµ‹è¯•ç»“æœ"""
    test_name: str
    duration_seconds: float
    total_requests: int
    successful_requests: int
    failed_requests: int
    metrics: List[PerformanceMetric]
    details: Optional[Dict[str, Any]] = None

class PerformanceBenchmark:
    """æ€§èƒ½åŸºå‡†æµ‹è¯•ç±»"""
    
    def __init__(self, webhook_url: str, encrypt_key: str, verification_token: str):
        self.webhook_url = webhook_url
        self.encrypt_key = encrypt_key
        self.verification_token = verification_token
        self.results: List[BenchmarkResult] = []
    
    async def run_all_benchmarks(self) -> Dict[str, Any]:
        """è¿è¡Œæ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•"""
        logger.info("å¼€å§‹æ€§èƒ½åŸºå‡†æµ‹è¯•...")
        start_time = time.time()
        
        benchmark_tests = [
            ("å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•", self.benchmark_response_time),
            ("å¹¶å‘å¤„ç†åŸºå‡†æµ‹è¯•", self.benchmark_concurrent_processing),
            ("ååé‡åŸºå‡†æµ‹è¯•", self.benchmark_throughput),
            ("è´Ÿè½½æµ‹è¯•", self.benchmark_load_test),
            ("å‹åŠ›æµ‹è¯•", self.benchmark_stress_test)
        ]
        
        for test_name, test_func in benchmark_tests:
            logger.info(f"æ‰§è¡ŒåŸºå‡†æµ‹è¯•: {test_name}")
            try:
                await test_func()
                logger.info(f"âœ… {test_name} å®Œæˆ")
            except Exception as e:
                logger.error(f"âŒ {test_name} å¤±è´¥: {e}")
                # æ·»åŠ å¤±è´¥ç»“æœ
                self.results.append(BenchmarkResult(
                    test_name=test_name,
                    duration_seconds=0,
                    total_requests=0,
                    successful_requests=0,
                    failed_requests=1,
                    metrics=[],
                    details={'error': str(e)}
                ))
        
        total_duration = time.time() - start_time
        return self.generate_benchmark_report(total_duration)
    
    async def benchmark_response_time(self):
        """å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•"""
        logger.info("å¼€å§‹å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•...")
        
        # æµ‹è¯•ä¸åŒç±»å‹çš„è¯·æ±‚
        test_cases = [
            ("URLéªŒè¯è¯·æ±‚", self.create_url_verification_payload),
            ("ç®€å•æ¶ˆæ¯è¯·æ±‚", self.create_simple_message_payload),
            ("å¤æ‚æ¶ˆæ¯è¯·æ±‚", self.create_complex_message_payload)
        ]
        
        all_response_times = []
        test_details = {}
        
        for case_name, payload_func in test_cases:
            response_times = []
            
            # æ¯ç§æƒ…å†µæµ‹è¯•10æ¬¡
            for i in range(10):
                payload = payload_func()
                start_time = time.time()
                
                try:
                    async with aiohttp.ClientSession() as session:
                        headers = self.create_signed_headers(payload)
                        async with session.post(
                            self.webhook_url,
                            json=payload,
                            headers=headers,
                            timeout=aiohttp.ClientTimeout(total=30)
                        ) as response:
                            await response.text()
                            response_time = (time.time() - start_time) * 1000
                            response_times.append(response_time)
                            all_response_times.append(response_time)
                            
                except Exception as e:
                    logger.warning(f"è¯·æ±‚å¤±è´¥: {e}")
                    continue
            
            if response_times:
                test_details[case_name] = {
                    'avg_response_time': statistics.mean(response_times),
                    'min_response_time': min(response_times),
                    'max_response_time': max(response_times),
                    'p95_response_time': statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times)
                }
        
        if all_response_times:
            metrics = [
                PerformanceMetric(
                    metric_name="å¹³å‡å“åº”æ—¶é—´",
                    value=statistics.mean(all_response_times),
                    unit="ms",
                    description="æ‰€æœ‰è¯·æ±‚çš„å¹³å‡å“åº”æ—¶é—´",
                    threshold=1000.0,
                    passed=statistics.mean(all_response_times) < 1000.0
                ),
                PerformanceMetric(
                    metric_name="P95å“åº”æ—¶é—´",
                    value=statistics.quantiles(all_response_times, n=20)[18] if len(all_response_times) >= 20 else max(all_response_times),
                    unit="ms",
                    description="95%è¯·æ±‚çš„å“åº”æ—¶é—´",
                    threshold=2000.0,
                    passed=(statistics.quantiles(all_response_times, n=20)[18] if len(all_response_times) >= 20 else max(all_response_times)) < 2000.0
                ),
                PerformanceMetric(
                    metric_name="æœ€å¤§å“åº”æ—¶é—´",
                    value=max(all_response_times),
                    unit="ms",
                    description="æœ€æ…¢è¯·æ±‚çš„å“åº”æ—¶é—´",
                    threshold=5000.0,
                    passed=max(all_response_times) < 5000.0
                )
            ]
            
            self.results.append(BenchmarkResult(
                test_name="å“åº”æ—¶é—´åŸºå‡†æµ‹è¯•",
                duration_seconds=0,
                total_requests=len(all_response_times),
                successful_requests=len(all_response_times),
                failed_requests=0,
                metrics=metrics,
                details=test_details
            ))
    
    async def benchmark_concurrent_processing(self):
        """å¹¶å‘å¤„ç†åŸºå‡†æµ‹è¯•"""
        logger.info("å¼€å§‹å¹¶å‘å¤„ç†åŸºå‡†æµ‹è¯•...")
        
        # æµ‹è¯•ä¸åŒå¹¶å‘çº§åˆ«
        concurrency_levels = [5, 10, 20, 50]
        
        for concurrency in concurrency_levels:
            logger.info(f"æµ‹è¯•å¹¶å‘çº§åˆ«: {concurrency}")
            
            start_time = time.time()
            successful_requests = 0
            failed_requests = 0
            response_times = []
            
            async with aiohttp.ClientSession() as session:
                tasks = []
                
                for i in range(concurrency):
                    payload = self.create_simple_message_payload()
                    task = self.send_request_async(session, payload)
                    tasks.append(task)
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        failed_requests += 1
                    else:
                        successful_requests += 1
                        response_times.append(result)
            
            duration = time.time() - start_time
            
            if response_times:
                metrics = [
                    PerformanceMetric(
                        metric_name=f"å¹¶å‘{concurrency}-æˆåŠŸç‡",
                        value=(successful_requests / (successful_requests + failed_requests)) * 100,
                        unit="%",
                        description=f"å¹¶å‘{concurrency}çš„æˆåŠŸç‡",
                        threshold=95.0,
                        passed=(successful_requests / (successful_requests + failed_requests)) * 100 >= 95.0
                    ),
                    PerformanceMetric(
                        metric_name=f"å¹¶å‘{concurrency}-å¹³å‡å“åº”æ—¶é—´",
                        value=statistics.mean(response_times),
                        unit="ms",
                        description=f"å¹¶å‘{concurrency}çš„å¹³å‡å“åº”æ—¶é—´",
                        threshold=2000.0,
                        passed=statistics.mean(response_times) < 2000.0
                    ),
                    PerformanceMetric(
                        metric_name=f"å¹¶å‘{concurrency}-ååé‡",
                        value=successful_requests / duration,
                        unit="req/s",
                        description=f"å¹¶å‘{concurrency}çš„ååé‡",
                        threshold=10.0,
                        passed=(successful_requests / duration) >= 10.0
                    )
                ]
                
                self.results.append(BenchmarkResult(
                    test_name=f"å¹¶å‘å¤„ç†åŸºå‡†æµ‹è¯•-{concurrency}",
                    duration_seconds=duration,
                    total_requests=concurrency,
                    successful_requests=successful_requests,
                    failed_requests=failed_requests,
                    metrics=metrics,
                    details={
                        'concurrency_level': concurrency,
                        'avg_response_time': statistics.mean(response_times),
                        'throughput': successful_requests / duration
                    }
                ))
    
    async def benchmark_throughput(self):
        """ååé‡åŸºå‡†æµ‹è¯•"""
        logger.info("å¼€å§‹ååé‡åŸºå‡†æµ‹è¯•...")
        
        # æŒç»­å‘é€è¯·æ±‚60ç§’
        test_duration = 60
        start_time = time.time()
        end_time = start_time + test_duration
        
        successful_requests = 0
        failed_requests = 0
        response_times = []
        
        async with aiohttp.ClientSession() as session:
            while time.time() < end_time:
                payload = self.create_simple_message_payload()
                
                try:
                    request_start = time.time()
                    headers = self.create_signed_headers(payload)
                    async with session.post(
                        self.webhook_url,
                        json=payload,
                        headers=headers,
                        timeout=aiohttp.ClientTimeout(total=10)
                    ) as response:
                        await response.text()
                        response_time = (time.time() - request_start) * 1000
                        response_times.append(response_time)
                        successful_requests += 1
                        
                except Exception as e:
                    failed_requests += 1
                    continue
                
                # çŸ­æš‚å»¶è¿Ÿé¿å…è¿‡åº¦å‹åŠ›
                await asyncio.sleep(0.1)
        
        actual_duration = time.time() - start_time
        
        if successful_requests > 0:
            metrics = [
                PerformanceMetric(
                    metric_name="å¹³å‡ååé‡",
                    value=successful_requests / actual_duration,
                    unit="req/s",
                    description="æ¯ç§’å¤„ç†çš„è¯·æ±‚æ•°",
                    threshold=5.0,
                    passed=(successful_requests / actual_duration) >= 5.0
                ),
                PerformanceMetric(
                    metric_name="æˆåŠŸç‡",
                    value=(successful_requests / (successful_requests + failed_requests)) * 100,
                    unit="%",
                    description="è¯·æ±‚æˆåŠŸç‡",
                    threshold=95.0,
                    passed=(successful_requests / (successful_requests + failed_requests)) * 100 >= 95.0
                ),
                PerformanceMetric(
                    metric_name="å¹³å‡å“åº”æ—¶é—´",
                    value=statistics.mean(response_times),
                    unit="ms",
                    description="å¹³å‡å“åº”æ—¶é—´",
                    threshold=1000.0,
                    passed=statistics.mean(response_times) < 1000.0
                )
            ]
            
            self.results.append(BenchmarkResult(
                test_name="ååé‡åŸºå‡†æµ‹è¯•",
                duration_seconds=actual_duration,
                total_requests=successful_requests + failed_requests,
                successful_requests=successful_requests,
                failed_requests=failed_requests,
                metrics=metrics,
                details={
                    'test_duration': test_duration,
                    'actual_duration': actual_duration,
                    'throughput': successful_requests / actual_duration
                }
            ))
    
    async def benchmark_load_test(self):
        """è´Ÿè½½æµ‹è¯•"""
        logger.info("å¼€å§‹è´Ÿè½½æµ‹è¯•...")
        
        # æ¨¡æ‹Ÿæ­£å¸¸è´Ÿè½½ï¼šæ¯ç§’10ä¸ªè¯·æ±‚ï¼ŒæŒç»­5åˆ†é’Ÿ
        test_duration = 300  # 5åˆ†é’Ÿ
        target_rps = 10  # æ¯ç§’10ä¸ªè¯·æ±‚
        
        start_time = time.time()
        successful_requests = 0
        failed_requests = 0
        response_times = []
        
        async with aiohttp.ClientSession() as session:
            for second in range(test_duration):
                second_start = time.time()
                
                # åœ¨è¿™ä¸€ç§’å†…å‘é€target_rpsä¸ªè¯·æ±‚
                tasks = []
                for i in range(target_rps):
                    payload = self.create_simple_message_payload()
                    task = self.send_request_async(session, payload)
                    tasks.append(task)
                
                results = await asyncio.gather(*tasks, return_exceptions=True)
                
                for result in results:
                    if isinstance(result, Exception):
                        failed_requests += 1
                    else:
                        successful_requests += 1
                        response_times.append(result)
                
                # ç­‰å¾…åˆ°ä¸‹ä¸€ç§’
                elapsed = time.time() - second_start
                if elapsed < 1.0:
                    await asyncio.sleep(1.0 - elapsed)
                
                if (second + 1) % 60 == 0:  # æ¯åˆ†é’ŸæŠ¥å‘Šä¸€æ¬¡è¿›åº¦
                    logger.info(f"è´Ÿè½½æµ‹è¯•è¿›åº¦: {second + 1}/{test_duration}ç§’")
        
        actual_duration = time.time() - start_time
        
        if successful_requests > 0:
            metrics = [
                PerformanceMetric(
                    metric_name="è´Ÿè½½æµ‹è¯•-å¹³å‡ååé‡",
                    value=successful_requests / actual_duration,
                    unit="req/s",
                    description="è´Ÿè½½æµ‹è¯•æœŸé—´çš„å¹³å‡ååé‡",
                    threshold=8.0,
                    passed=(successful_requests / actual_duration) >= 8.0
                ),
                PerformanceMetric(
                    metric_name="è´Ÿè½½æµ‹è¯•-æˆåŠŸç‡",
                    value=(successful_requests / (successful_requests + failed_requests)) * 100,
                    unit="%",
                    description="è´Ÿè½½æµ‹è¯•æœŸé—´çš„æˆåŠŸç‡",
                    threshold=99.0,
                    passed=(successful_requests / (successful_requests + failed_requests)) * 100 >= 99.0
                ),
                PerformanceMetric(
                    metric_name="è´Ÿè½½æµ‹è¯•-P95å“åº”æ—¶é—´",
                    value=statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times),
                    unit="ms",
                    description="è´Ÿè½½æµ‹è¯•æœŸé—´95%è¯·æ±‚çš„å“åº”æ—¶é—´",
                    threshold=2000.0,
                    passed=(statistics.quantiles(response_times, n=20)[18] if len(response_times) >= 20 else max(response_times)) < 2000.0
                )
            ]
            
            self.results.append(BenchmarkResult(
                test_name="è´Ÿè½½æµ‹è¯•",
                duration_seconds=actual_duration,
                total_requests=successful_requests + failed_requests,
                successful_requests=successful_requests,
                failed_requests=failed_requests,
                metrics=metrics,
                details={
                    'target_rps': target_rps,
                    'test_duration': test_duration,
                    'actual_throughput': successful_requests / actual_duration
                }
            ))
    
    async def benchmark_stress_test(self):
        """å‹åŠ›æµ‹è¯•"""
        logger.info("å¼€å§‹å‹åŠ›æµ‹è¯•...")
        
        # é€æ­¥å¢åŠ è´Ÿè½½ç›´åˆ°ç³»ç»Ÿå¼€å§‹å‡ºç°é”™è¯¯
        max_concurrency = 100
        step_size = 10
        step_duration = 30  # æ¯ä¸ªçº§åˆ«æŒç»­30ç§’
        
        for concurrency in range(step_size, max_concurrency + 1, step_size):
            logger.info(f"å‹åŠ›æµ‹è¯• - å¹¶å‘çº§åˆ«: {concurrency}")
            
            start_time = time.time()
            successful_requests = 0
            failed_requests = 0
            response_times = []
            
            async with aiohttp.ClientSession() as session:
                # åœ¨step_durationæ—¶é—´å†…ä¿æŒconcurrencyçº§åˆ«çš„å¹¶å‘
                end_time = start_time + step_duration
                
                while time.time() < end_time:
                    tasks = []
                    for i in range(concurrency):
                        payload = self.create_simple_message_payload()
                        task = self.send_request_async(session, payload)
                        tasks.append(task)
                    
                    results = await asyncio.gather(*tasks, return_exceptions=True)
                    
                    for result in results:
                        if isinstance(result, Exception):
                            failed_requests += 1
                        else:
                            successful_requests += 1
                            response_times.append(result)
                    
                    # çŸ­æš‚å»¶è¿Ÿ
                    await asyncio.sleep(1)
            
            actual_duration = time.time() - start_time
            error_rate = (failed_requests / (successful_requests + failed_requests)) * 100 if (successful_requests + failed_requests) > 0 else 100
            
            metrics = [
                PerformanceMetric(
                    metric_name=f"å‹åŠ›æµ‹è¯•-{concurrency}-é”™è¯¯ç‡",
                    value=error_rate,
                    unit="%",
                    description=f"å¹¶å‘{concurrency}æ—¶çš„é”™è¯¯ç‡",
                    threshold=10.0,
                    passed=error_rate < 10.0
                )
            ]
            
            if response_times:
                metrics.append(PerformanceMetric(
                    metric_name=f"å‹åŠ›æµ‹è¯•-{concurrency}-å¹³å‡å“åº”æ—¶é—´",
                    value=statistics.mean(response_times),
                    unit="ms",
                    description=f"å¹¶å‘{concurrency}æ—¶çš„å¹³å‡å“åº”æ—¶é—´",
                    threshold=5000.0,
                    passed=statistics.mean(response_times) < 5000.0
                ))
            
            self.results.append(BenchmarkResult(
                test_name=f"å‹åŠ›æµ‹è¯•-{concurrency}",
                duration_seconds=actual_duration,
                total_requests=successful_requests + failed_requests,
                successful_requests=successful_requests,
                failed_requests=failed_requests,
                metrics=metrics,
                details={
                    'concurrency_level': concurrency,
                    'error_rate': error_rate,
                    'avg_response_time': statistics.mean(response_times) if response_times else 0
                }
            ))
            
            # å¦‚æœé”™è¯¯ç‡è¶…è¿‡50%ï¼Œåœæ­¢å‹åŠ›æµ‹è¯•
            if error_rate > 50:
                logger.warning(f"é”™è¯¯ç‡è¿‡é«˜({error_rate:.2f}%)ï¼Œåœæ­¢å‹åŠ›æµ‹è¯•")
                break
    
    async def send_request_async(self, session: aiohttp.ClientSession, payload: dict) -> float:
        """å¼‚æ­¥å‘é€è¯·æ±‚å¹¶è¿”å›å“åº”æ—¶é—´"""
        start_time = time.time()
        
        headers = self.create_signed_headers(payload)
        async with session.post(
            self.webhook_url,
            json=payload,
            headers=headers,
            timeout=aiohttp.ClientTimeout(total=30)
        ) as response:
            await response.text()
            return (time.time() - start_time) * 1000
    
    def create_url_verification_payload(self) -> dict:
        """åˆ›å»ºURLéªŒè¯è½½è·"""
        return {
            "header": {
                "event_type": "url_verification"
            },
            "challenge": f"test_challenge_{int(time.time())}",
            "token": self.verification_token,
            "type": "url_verification"
        }
    
    def create_simple_message_payload(self) -> dict:
        """åˆ›å»ºç®€å•æ¶ˆæ¯è½½è·"""
        return {
            "header": {
                "event_type": "im.message.receive_v1",
                "app_id": "cli_test_app"
            },
            "event": {
                "sender": {
                    "sender_type": "user",
                    "sender_id": {"user_id": f"test_user_{int(time.time())}"}
                },
                "message": {
                    "message_id": f"test_message_{int(time.time())}",
                    "chat_id": f"test_chat_{int(time.time())}",
                    "message_type": "text",
                    "content": json.dumps({"text": "æµ‹è¯•æ¶ˆæ¯"})
                }
            }
        }
    
    def create_complex_message_payload(self) -> dict:
        """åˆ›å»ºå¤æ‚æ¶ˆæ¯è½½è·"""
        return {
            "header": {
                "event_type": "im.message.receive_v1",
                "app_id": "cli_test_app"
            },
            "event": {
                "sender": {
                    "sender_type": "user",
                    "sender_id": {"user_id": f"test_user_{int(time.time())}"}
                },
                "message": {
                    "message_id": f"test_message_{int(time.time())}",
                    "chat_id": f"test_chat_{int(time.time())}",
                    "message_type": "text",
                    "content": json.dumps({
                        "text": "è¿™æ˜¯ä¸€ä¸ªå¤æ‚çš„æµ‹è¯•æ¶ˆæ¯ï¼ŒåŒ…å«æ›´å¤šå†…å®¹å’Œæ•°æ®ï¼Œç”¨äºæµ‹è¯•ç³»ç»Ÿå¤„ç†å¤æ‚æ¶ˆæ¯çš„æ€§èƒ½ã€‚" * 10
                    }),
                    "mentions": [
                        {
                            "key": "@_user_1",
                            "id": {"user_id": "user_123"},
                            "name": "Test User"
                        }
                    ]
                }
            }
        }
    
    def create_signed_headers(self, payload: dict) -> dict:
        """åˆ›å»ºå¸¦ç­¾åçš„è¯·æ±‚å¤´"""
        timestamp = str(int(time.time()))
        nonce = f"test_nonce_{int(time.time() * 1000)}"
        body = json.dumps(payload)
        
        string_to_sign = f"{timestamp}{nonce}{self.encrypt_key}{body}"
        signature = hashlib.sha256(string_to_sign.encode()).hexdigest()
        
        return {
            'Content-Type': 'application/json',
            'X-Lark-Request-Timestamp': timestamp,
            'X-Lark-Request-Nonce': nonce,
            'X-Lark-Signature': signature
        }
    
    def generate_benchmark_report(self, total_duration: float) -> Dict[str, Any]:
        """ç”ŸæˆåŸºå‡†æµ‹è¯•æŠ¥å‘Š"""
        total_tests = len(self.results)
        passed_tests = sum(1 for result in self.results 
                          if all(metric.passed for metric in result.metrics if metric.passed is not None))
        
        total_requests = sum(result.total_requests for result in self.results)
        total_successful = sum(result.successful_requests for result in self.results)
        total_failed = sum(result.failed_requests for result in self.results)
        
        overall_success_rate = (total_successful / total_requests * 100) if total_requests > 0 else 0
        
        report = {
            'benchmark_summary': {
                'total_tests': total_tests,
                'passed_tests': passed_tests,
                'failed_tests': total_tests - passed_tests,
                'total_duration_seconds': total_duration,
                'total_requests': total_requests,
                'successful_requests': total_successful,
                'failed_requests': total_failed,
                'overall_success_rate': f"{overall_success_rate:.2f}%",
                'timestamp': datetime.utcnow().isoformat()
            },
            'test_results': [
                {
                    'test_name': result.test_name,
                    'duration_seconds': result.duration_seconds,
                    'total_requests': result.total_requests,
                    'successful_requests': result.successful_requests,
                    'failed_requests': result.failed_requests,
                    'metrics': [
                        {
                            'metric_name': metric.metric_name,
                            'value': metric.value,
                            'unit': metric.unit,
                            'description': metric.description,
                            'threshold': metric.threshold,
                            'passed': metric.passed
                        }
                        for metric in result.metrics
                    ],
                    'details': result.details
                }
                for result in self.results
            ],
            'performance_summary': self._generate_performance_summary()
        }
        
        return report
    
    def _generate_performance_summary(self) -> Dict[str, Any]:
        """ç”Ÿæˆæ€§èƒ½æ‘˜è¦"""
        all_metrics = []
        for result in self.results:
            all_metrics.extend(result.metrics)
        
        # æŒ‰æŒ‡æ ‡ç±»å‹åˆ†ç»„
        response_time_metrics = [m for m in all_metrics if 'å“åº”æ—¶é—´' in m.metric_name]
        throughput_metrics = [m for m in all_metrics if 'ååé‡' in m.metric_name]
        success_rate_metrics = [m for m in all_metrics if 'æˆåŠŸç‡' in m.metric_name]
        
        summary = {}
        
        if response_time_metrics:
            summary['response_time'] = {
                'avg_response_time': statistics.mean([m.value for m in response_time_metrics]),
                'min_response_time': min([m.value for m in response_time_metrics]),
                'max_response_time': max([m.value for m in response_time_metrics])
            }
        
        if throughput_metrics:
            summary['throughput'] = {
                'avg_throughput': statistics.mean([m.value for m in throughput_metrics]),
                'max_throughput': max([m.value for m in throughput_metrics])
            }
        
        if success_rate_metrics:
            summary['reliability'] = {
                'avg_success_rate': statistics.mean([m.value for m in success_rate_metrics]),
                'min_success_rate': min([m.value for m in success_rate_metrics])
            }
        
        return summary

def main():
    """ä¸»å‡½æ•°"""
    parser = argparse.ArgumentParser(description='é£ä¹¦æœºå™¨äººç³»ç»Ÿæ€§èƒ½åŸºå‡†æµ‹è¯•')
    parser.add_argument('--webhook-url', required=True, help='Webhook URL')
    parser.add_argument('--encrypt-key', required=True, help='é£ä¹¦åŠ å¯†å¯†é’¥')
    parser.add_argument('--verification-token', required=True, help='é£ä¹¦éªŒè¯Token')
    parser.add_argument('--output', default='performance_benchmark.json', help='åŸºå‡†æµ‹è¯•æŠ¥å‘Šè¾“å‡ºæ–‡ä»¶')
    
    args = parser.parse_args()
    
    async def run_benchmark():
        benchmark = PerformanceBenchmark(
            webhook_url=args.webhook_url,
            encrypt_key=args.encrypt_key,
            verification_token=args.verification_token
        )
        
        return await benchmark.run_all_benchmarks()
    
    # è¿è¡ŒåŸºå‡†æµ‹è¯•
    report = asyncio.run(run_benchmark())
    
    # è¾“å‡ºæŠ¥å‘Š
    with open(args.output, 'w', encoding='utf-8') as f:
        json.dump(report, f, indent=2, ensure_ascii=False)
    
    # æ‰“å°æ‘˜è¦
    summary = report['benchmark_summary']
    print(f"\n{'='*60}")
    print(f"æ€§èƒ½åŸºå‡†æµ‹è¯•æŠ¥å‘Š")
    print(f"{'='*60}")
    print(f"æ€»æµ‹è¯•æ•°: {summary['total_tests']}")
    print(f"é€šè¿‡æµ‹è¯•: {summary['passed_tests']}")
    print(f"å¤±è´¥æµ‹è¯•: {summary['failed_tests']}")
    print(f"æ€»è¯·æ±‚æ•°: {summary['total_requests']}")
    print(f"æˆåŠŸè¯·æ±‚: {summary['successful_requests']}")
    print(f"å¤±è´¥è¯·æ±‚: {summary['failed_requests']}")
    print(f"æ•´ä½“æˆåŠŸç‡: {summary['overall_success_rate']}")
    print(f"æ€»è€—æ—¶: {summary['total_duration_seconds']:.2f}ç§’")
    print(f"æŠ¥å‘Šæ–‡ä»¶: {args.output}")
    
    # æ‰“å°æ€§èƒ½æ‘˜è¦
    if 'performance_summary' in report:
        perf_summary = report['performance_summary']
        print(f"\næ€§èƒ½æ‘˜è¦:")
        
        if 'response_time' in perf_summary:
            rt = perf_summary['response_time']
            print(f"  å“åº”æ—¶é—´: å¹³å‡{rt['avg_response_time']:.2f}ms, æœ€å°{rt['min_response_time']:.2f}ms, æœ€å¤§{rt['max_response_time']:.2f}ms")
        
        if 'throughput' in perf_summary:
            tp = perf_summary['throughput']
            print(f"  ååé‡: å¹³å‡{tp['avg_throughput']:.2f}req/s, æœ€å¤§{tp['max_throughput']:.2f}req/s")
        
        if 'reliability' in perf_summary:
            rel = perf_summary['reliability']
            print(f"  å¯é æ€§: å¹³å‡æˆåŠŸç‡{rel['avg_success_rate']:.2f}%, æœ€ä½æˆåŠŸç‡{rel['min_success_rate']:.2f}%")
    
    if summary['failed_tests'] > 0:
        print(f"\nâš ï¸  æœ‰ {summary['failed_tests']} ä¸ªæµ‹è¯•æœªé€šè¿‡åŸºå‡†è¦æ±‚")
        exit(1)
    else:
        print(f"\nğŸ‰ æ‰€æœ‰æ€§èƒ½åŸºå‡†æµ‹è¯•é€šè¿‡!")
        exit(0)

if __name__ == '__main__':
    main()